# COMP30023 Assignment 1
# Author: Chris Bradley (635 847)

###########
# Results #
###########
# Experiment 1: Many processes of the same size
# Memory size: 20
# Input file: exp1_same.in
+-----------+--------------------+----------------+--------------------------+
| Algorithm | Average #Processes | Average #Holes | Average Memory Usage (%) |
+-----------+--------------------+----------------+--------------------------+
| Best      | 13.60              | 0.19           | 90.73                    |
+-----------+--------------------+----------------+--------------------------+
| First     | 13.60              | 0.19           | 90.73                    |
+-----------+--------------------+----------------+--------------------------+
| Next      | 13.60              | 0.19           | 90.73                    |
+-----------+--------------------+----------------+--------------------------+
| Worst     | 13.60              | 0.19           | 90.73                    |
+-----------+--------------------+----------------+--------------------------+

# Experiment 2: Processes of increasing size
# Memory size: 2560
# Input file: exp2_increasing.in 
+-----------+--------------------+----------------+--------------------------+
| Algorithm | Average #Processes | Average #Holes | Average Memory Usage (%) |
+-----------+--------------------+----------------+--------------------------+
| Best      | 7.67               | 1.00           | 41.33                    |
+-----------+--------------------+----------------+--------------------------+
| First     | 7.67               | 1.00           | 41.33                    |
+-----------+--------------------+----------------+--------------------------+
| Next      | 7.67               | 1.00           | 41.33                    |
+-----------+--------------------+----------------+--------------------------+
| Worst     | 7.67               | 1.00           | 41.33                    |
+-----------+--------------------+----------------+--------------------------+

# Experiment 3: Small->Big->Small->Big process ordering
# Memory size: 12
# Input file: exp3_alternating.in
+-----------+--------------------+----------------+------------------------+
| Algorithm | Average #Processes | Average #Holes | Final Memory Usage (%) |
+-----------+--------------------+----------------+------------------------+
| Best      | 4.79               | 1.32           | 73.16                  |
+-----------+--------------------+----------------+------------------------+
| First     | 5.20               | 0.87           | 79.67                  |
+-----------+--------------------+----------------+------------------------+
| Next      | 4.79               | 1.32           | 73.16                  |
+-----------+--------------------+----------------+------------------------+
| Worst     | 4.62               | 1.29           | 73.33                  |
+-----------+--------------------+----------------+------------------------+

##############
# Discussion #
##############
The first experiment was designed to show what happens when many processes of the same size are loaded.
As the results show this results in all algorithms behaving in exactly the same way as they do not have multiple
holes to choose between, they always have a single hole.

The second experiment was designed to illustrate what happens when there are a few large processes and many
smaller processes. Once again in this experiment all three algorithms behave exactly the same. This occurs
as memory is loaded with the small processes, however before any holes are allowed to form they are kicked
out of memory by a large process entering, effectively clearing memory and preventing fragmentation.

The third experiment was designed to illustrate what many similar, but not the same, sized proceesses
will do to memory fragmentation. This is the only scenario where allocation algorithms seem to affect
performance.
Here 'first' allocation performs the best with lowest memory holes and highest average memory usage.
The other three algorithms perform similarly, despite the names of 'worst' and 'best' implying
a potential large difference, they both performed similarly in the end.

Overall across all three experiments, the differences between the four algorithms were minimal.

###########
# Answers #
###########
# Question 1
This statement is largely accurate. In a typical workload a system will be loaded by many processes of similar sizes, meaning any holes
left in memory will be of similar size. This means that the algorithm used for selecting which hole to insert a new process into
becomes less important as fragmentation doesn't really change between different decisions.
This statement is supported by the results of my experiments, in two of the three experiments all the algorithms performed exactly the same.
In the third scenario three of the algorithms performed quite similarly with only first showing a (small) improvement.
It is also important to consider that computers often see varied workloads. Each algorithm given here performs optimally in dfferent
circumstances and a computer is likely to meet these circumstrances at different times during its uptime, having the effect of
cancelling out any real gains by usng a particular allocation strategy as gains in one area are losses in others.

# Question 2
Assuming N processes are in memory, the maximum number of holes in memory would be N + 1. This is easy to see as theoretiaclly
each process could be surrounded by free space. This would be incredibly unlikely though and indicative of a bad allocation
strategy.

Assume:
1. Processes swapped in will be placed at the beginning or end of free space to minimize free 'hole' creation
2. Memory has been loaded and has reached an equlibrium (no excess swapping occuring)
3. Biggest process is max an order of magnitude bigger than the smallest one. Overly large processes wil
   kick all small ones out of memory, skewing results as they will be swapped back in
   sequentially, resulting in minimal holes.

To explain the rule Sh = Sm / 2, here's an example. There are 4 processes in memory with no free space at time 0.
0: P P P P

At time 1 a new process arrives so the first process is swapped out. This new process is slightly smaller
so there is now free space.

1: PF P P P

At time 2 a new process comes into memory so to make room process three is swapped out, once again ths new process is smaller.

2: PF P PF P

Now there are 4 processes and 2 holes. Every process in memory is now adjacent to a block of free memory
so no matter which process is swapped out next, its memory wll be consolidated into one block instead of forming a new hole.
Here Sh = 2 and Sm = 4, thereby satisfying the formula Sh= Sm / 2 (Sh = 4 / 2 => 2)

Of course, as dicussed earlier, if swaps are completed in a different order then we may end up with higher fragmentation and thus more
holes, however on average the above should hold true.
