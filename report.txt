# COMP30023 Assignment 1
# Question 1
This statement is largely accurate. In a typical workload a system will be loaded by many processes of similar sizes, meaning any holes
left in memory will be of similar size. This means that the algorithm used for selecting which hole to insert a new process into
becomes less important overtime as there are few differences.

# Question 2
We're talking about the average case so we'll talk about an average scenario, memory that has already been loaded and has reached 'equilibrium'.
While memory is being initially loaded this will likely not hold true as we load processes in sequentially. Until swapping has occured this does
not become very relevant.

Assuming we have P processes in memory, the maximum number of holes in memory would be P + 1. This is entirely worst cast, disaster, end of the world
type stuff though where memory has become so fragmented we should probably just give up and start again.

On the average case lets assume that we're employing some sort of remotely intelligent swapping algorithm. When processes are swapped in they will be
placed at the beginning or the end of a block of free space. Without this assumpting we'll probably end up at our dooms day scenario.
Additionally let's assume that all processes are of a similar size with the maximum process being at max an order of magnitude bigger than the smallest ones.
We don't want any massive processes coming and kicking all the small ones out of memory, that'd end up making one massive hole and skew our statistics while
trying to reload smaller ones.

So, let's talk about our average case. Many similarly sized processes having reached an equilibrium state where processes have been swapped in and out.



		F	P
F		1	0
PF		1	1
PPF		1	2
PPPF		1	3	
PPPP		0	4	
PFPPP		1	4
PFPFPP		2	4
PFPFPFP		3	4
PFPFPFPF	4	4
FPFPFPFPF	5	4

Min: 1
Max: P + 1
